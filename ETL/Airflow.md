**Apache Airflow** — это платформа для программного создания, планирования и мониторинга рабочих процессов.

В Airflow рабочие процессы проектируются и выражаются в виде DAG, где каждый шаг - отдельная задача.

++Простой в использовании интерфейс; Независимый планировщик (можно разделять таски от cron; DAGи работают с тасками двух категорий: сенсоры и операторы); Чистый Python; Операторы (дефолные и кастомные); Функциональные принципы; Открытый исходный код

--Неинтуитивное моделирование данных, сложность изменения интервала расписания, отсутствие встроенной поддержки Windows, непростые задачи CI/CD.

Ключевые принципы Airflow: динамичность (Конвейеры Airflow представляют собой конфигурацию в виде кода (Python)), расширяемость, элегантность (шаблоны Jinja) и масштабируемость.

Cron (устарел) — это программа для запуска команд по расписанию, например, для резервных копий, удаления старых файлов и обслуживания системы.

**Конвейер Airflow** — это просто скрипт на Python, который определяет объект DAG Airflow.

**Аналоги** Apache Airflow: Prefect (проще в настройке), Dagster (акцент на тестируемость и удобство), Oozie (для Hadoop), Databricks Workflows, Azure Data Factory, AWS Step Functions.



&nbsp;	**Концепции Airflow**

1\\ DAG — это схема, которая показывает, какие задачи нужно выполнить и в каком порядке.

2\\ Область видимости - Airflow загружает DAG из файла, если он доступен.

3\\ Аргументы по умолчанию - можно задать общие параметры для всех задач DAG, чтобы не повторять их вручную.

4\\ Запуск DAG — это выполнение всех задач DAG для определённой даты.

5\\ execution\_date - это дата и время, для которых запускается DAG и задачи.

6\\ Задача - это отдельная работа внутри DAG, написанная на Python.

7\\ Экземпляр задачи — это конкретный запуск задачи для определённой даты. У него есть статус: выполняется, успех, ошибка и другие.

8\\ Task Lifecycle - задача проходит разные этапы: успех, ошибка, выполнение, пропущено и т.д. Это видно в интерфейсе Airflow.

9\\ Операторы - определяют, что именно делает задача.

10\\ Назначение DAG - операторы можно назначать DAG. После назначения оператор нельзя перенести.

11\\ XComs - позволяют задачам обмениваться данными между собой.



&nbsp;	**Компоненты архитектуры Airflow:**

Архитектура Airflow: одноузловая архитектура (одна машина) и многоузловая архитектура (сеть машин).

Работающий экземпляр Airflow имеет ряд демонов, которые работают вместе:

1\\ **Веб-сервер** - позволяет управлять Airflow через браузер (HTTP-запросы и Python Flask Web Application): запускать задачи, смотреть их состояние, управлять пользователями и настройками.

2\\ **Планировщик** - решает, когда запускать задачи и DAG-графы по расписанию. Следит за их выполнением и отправляет задачи на исполнение.

3\\ **Executors** (механизм)/ **Workers**(узел или процессор) - Выполняют задачи DAG. Бывают разные типы:

Последовательный — задачи выполняются по одной.

Локальный — задачи могут выполняться параллельно.

Celery — задачи распределяются между разными машинами для масштабируемости.

4\\ **Metastore** - хранит настройки, пользователей, роли и информацию о DAG и задачах.



 	**DAGs**

Airflow позволяет создавать и планировать конвейеры задач, используя DAG. DAG состоит из операторов и зависимостей между ними.

Directed - Если существует несколько задач, каждая должна иметь как минимум одну определенную задачу, предшествующую или следующую за ней.

Ациклическая - Задачи не должны создавать данные, которые затем ссылаются на себя. Это необходимо для предотвращения создания бесконечных циклов.

Графовая - Все задачи имеют четкую структуру, процессы происходят в четких точках с заданными связями с другими задачами.

DAG определяется в скрипте Python и представляет структуру DAG (задачи и их зависимости) в виде кода.

Цель ДАГ - обеспечить выполнение всех действий в нужное время, в нужном порядке или с правильной обработкой любых непредвиденных проблем.

*ПР: from airflow import DAG*

*dag = DAG(  dag\_id='example\_bash\_operator',   schedule\_interval='0 0 \* \* \*',   dagrun\_timeout=timedelta(minutes=60),   tags=\['example'] )*



	**Operators, Hooks, and Sensors**

**Операторы** — отвечают за то, что именно делает таска (пр: запуск скрипта, отправка письма или выполнение SQL-запроса).

BashOperator — выполняет bash-команды

PythonOperator — запускает Python-функцию

EmailOperator — отправляет письма

SimpleHttpOperator — делает HTTP-запрос

SQL-операторы — работают с базами данных

*ПР: process\_data = PythonOperator( task\_id='process\_data', python\_callable=process\_file )*

**Сенсоры** — это операторы, которые ждут какого-то события (пр: появления файла или завершения другой задачи).

ExternalTaskSensor — ждёт другую задачу

HivePartitionSensor — ждёт раздел в Hive

S3KeySensor — ждёт файл в S3

*ПР: wait\_for\_file = FileSensor( task\_id='wait\_for\_file', filepath='/tmp/input\_file.txt', poke\_interval=10, # every 10s    timeout=300,  # Wait for up to 5 minutes  )*

**Хуки** — это интерфейсы для подключения к внешним сервисам (пр: S3, MySQL). Операторы используют хуки для работы с этими сервисами.

*ПР: s3\_hook = S3Hook(aws\_conn\_id='aws\_default')*

 	**Tasks, Task Instances, and Lifecycle**

**Задача** — это действие, которое описано в DAG (пр: запуск команды). Задача - любой экземпляр оператора.

*ПР: run\_this = BashOperator( task\_id='run\_after\_loop',  bash\_command='echo 1',  dag=dag, )*

**Экземпляр задачи** — это конкретный запуск задачи при выполнении DAG.

**Жизненный цикл задачи** (основные этапы): Нет статуса => Запланировано => В очереди => Выполняется => Успешно



	**XCom в Airflow**
XCom позволяет задачам обмениваться данными между собой. XCom — это просто сообщение с ключом, значением и временем. Задача может отправить данные через xcom\_push(), а получить — через xcom\_pull(). Если задача возвращает значение, оно автоматически сохраняется в XCom. Можно получить XCom по ключу, имени задачи или DAG. Если указать одну задачу — вернётся последнее значение; если список задач — список значений.



	<b>Пример базового определения конвейера:</b>

*from datetime import timedelta*

*from airflow import DAG*

*from airflow.operators.bash\_operator import BashOperator*

*from airflow.utils.dates import days\_ago*



*// определим словарь параметров по умолчанию для создания задач:*

*default\_args = {*

*    'owner': 'airflow',*

*    'depends\_on\_past': False,*

*    'start\_date': days\_ago(2),*

*    'email': \['airflow@example.com'],*

*    'email\_on\_failure': False,*

*    'email\_on\_retry': False,*

*    'retries': 1,*

*    'retry\_delay': timedelta(minutes=5), }*



*// Создание объекта DAG*

*dag = DAG(*

*    'tutorial',*

*    default\_args=default\_args,*

*    description='A simple tutorial DAG',*

*    schedule\_interval=timedelta(days=1), )*



*// генерируем задачи:*

*t1 = BashOperator(*

*    task\_id='print\_date',*

*    bash\_command='date',*

*    dag=dag, )*

*t2 = BashOperator(*

*    task\_id='sleep',*

*    depends\_on\_past=False,*

*    bash\_command='sleep 5',*

*    retries=3,*

*    dag=dag, )*



*// Использование шаблонов Jinja*

*templated\_command = """*

*{% for i in range(5) %}*

*    echo "{{ ds }}"*

*    echo "{{ macros.ds\_add(ds, 7)}}"*

*    echo "{{ params.my\_param }}"*

*{% endfor %}  """*

*t3 = BashOperator(*

*    task\_id='templated',*

*    depends\_on\_past=False,*

*    bash\_command=templated\_command,*

*    params={'my\_param': 'Parameter I passed in'},*

*    dag=dag,  )*



*// Настройка зависимостей*

*t1 >> \[t2, t3]*







